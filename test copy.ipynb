{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/KS_MRFN_AGE_ACCTO_RECOMMEND_SPORTS_INFO_202303.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_mapping(weight_text):\n",
    "    if weight_text == \"저체중\":\n",
    "        return -2.5\n",
    "    elif weight_text == \"정상\":\n",
    "        return 0\n",
    "    elif weight_text == \"비만전단계비만\":\n",
    "        return 1\n",
    "    elif weight_text == \"1단계비만\":\n",
    "        return 2\n",
    "    elif weight_text == \"2단계비만\":\n",
    "        return 3\n",
    "    elif weight_text == \"3단계비만\":\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sex_mapping(sex):\n",
    "    if sex == \"F\":\n",
    "        return -1\n",
    "    if sex == \"M\":\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {}\n",
    "for i in (list(data['RECOMEND_MVM_NM'])):\n",
    "    \n",
    "    if ( \"동적\" in i): continue\n",
    "    if ( \"스트레칭\" in i): continue\n",
    "    \n",
    "    if i not in header:\n",
    "        header[i] = 0\n",
    "    \n",
    "    header[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by value\n",
    "header = dict(sorted(header.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Get the top 10 sports\n",
    "top_10_sports = list(header.keys())[:10]\n",
    "\n",
    "filtered_data = data[data['RECOMEND_MVM_NM'].isin(top_10_sports)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGRDE_FLAG_NM</th>\n",
       "      <th>BMI_IDEX_GRAD_NM</th>\n",
       "      <th>MBER_SEXDSTN_FLAG_CD</th>\n",
       "      <th>COAW_FLAG_NM</th>\n",
       "      <th>SPORTS_STEP_NM</th>\n",
       "      <th>FLAG_ACCTO_RECOMEND_MVM_RANK_CO</th>\n",
       "      <th>RECOMEND_MVM_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10대</td>\n",
       "      <td>정상</td>\n",
       "      <td>F</td>\n",
       "      <td>참가증</td>\n",
       "      <td>준비운동</td>\n",
       "      <td>2</td>\n",
       "      <td>몸통 비틀기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10대</td>\n",
       "      <td>정상</td>\n",
       "      <td>F</td>\n",
       "      <td>1등급</td>\n",
       "      <td>준비운동</td>\n",
       "      <td>2</td>\n",
       "      <td>몸통 비틀기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10대</td>\n",
       "      <td>정상</td>\n",
       "      <td>F</td>\n",
       "      <td>2등급</td>\n",
       "      <td>준비운동</td>\n",
       "      <td>2</td>\n",
       "      <td>몸통 비틀기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10대</td>\n",
       "      <td>정상</td>\n",
       "      <td>F</td>\n",
       "      <td>3등급</td>\n",
       "      <td>준비운동</td>\n",
       "      <td>2</td>\n",
       "      <td>몸통 비틀기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10대</td>\n",
       "      <td>정상</td>\n",
       "      <td>M</td>\n",
       "      <td>참가증</td>\n",
       "      <td>준비운동</td>\n",
       "      <td>3</td>\n",
       "      <td>몸통 비틀기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>70대 이상</td>\n",
       "      <td>3단계비만</td>\n",
       "      <td>M</td>\n",
       "      <td>1등급</td>\n",
       "      <td>본운동</td>\n",
       "      <td>4</td>\n",
       "      <td>엎드려 양팔 및 다리 들어올리기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>70대 이상</td>\n",
       "      <td>3단계비만</td>\n",
       "      <td>M</td>\n",
       "      <td>2등급</td>\n",
       "      <td>본운동</td>\n",
       "      <td>1</td>\n",
       "      <td>몸통 들어올리기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>70대 이상</td>\n",
       "      <td>3단계비만</td>\n",
       "      <td>M</td>\n",
       "      <td>2등급</td>\n",
       "      <td>본운동</td>\n",
       "      <td>5</td>\n",
       "      <td>엎드려 양팔 및 다리 들어올리기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>70대 이상</td>\n",
       "      <td>3단계비만</td>\n",
       "      <td>M</td>\n",
       "      <td>3등급</td>\n",
       "      <td>본운동</td>\n",
       "      <td>1</td>\n",
       "      <td>몸통 들어올리기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>70대 이상</td>\n",
       "      <td>3단계비만</td>\n",
       "      <td>M</td>\n",
       "      <td>3등급</td>\n",
       "      <td>본운동</td>\n",
       "      <td>3</td>\n",
       "      <td>엎드려 양팔 및 다리 들어올리기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>733 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGRDE_FLAG_NM BMI_IDEX_GRAD_NM MBER_SEXDSTN_FLAG_CD COAW_FLAG_NM  \\\n",
       "6              10대               정상                    F          참가증   \n",
       "21             10대               정상                    F          1등급   \n",
       "36             10대               정상                    F          2등급   \n",
       "51             10대               정상                    F          3등급   \n",
       "67             10대               정상                    M          참가증   \n",
       "...            ...              ...                  ...          ...   \n",
       "4998        70대 이상            3단계비만                    M          1등급   \n",
       "5010        70대 이상            3단계비만                    M          2등급   \n",
       "5014        70대 이상            3단계비만                    M          2등급   \n",
       "5025        70대 이상            3단계비만                    M          3등급   \n",
       "5027        70대 이상            3단계비만                    M          3등급   \n",
       "\n",
       "     SPORTS_STEP_NM  FLAG_ACCTO_RECOMEND_MVM_RANK_CO    RECOMEND_MVM_NM  \n",
       "6              준비운동                                2             몸통 비틀기  \n",
       "21             준비운동                                2             몸통 비틀기  \n",
       "36             준비운동                                2             몸통 비틀기  \n",
       "51             준비운동                                2             몸통 비틀기  \n",
       "67             준비운동                                3             몸통 비틀기  \n",
       "...             ...                              ...                ...  \n",
       "4998            본운동                                4  엎드려 양팔 및 다리 들어올리기  \n",
       "5010            본운동                                1           몸통 들어올리기  \n",
       "5014            본운동                                5  엎드려 양팔 및 다리 들어올리기  \n",
       "5025            본운동                                1           몸통 들어올리기  \n",
       "5027            본운동                                3  엎드려 양팔 및 다리 들어올리기  \n",
       "\n",
       "[733 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['실내 자전거타기',\n",
       " '달리기',\n",
       " '다리 벌려 앞으로 상체 숙이기',\n",
       " '앉았다 일어서기',\n",
       " '버피운동',\n",
       " '몸통 비틀기',\n",
       " '다리 벌려 옆으로 상체 숙이기',\n",
       " '발목 얹고 다리 잡아당기기',\n",
       " '엎드려 양팔 및 다리 들어올리기',\n",
       " '몸통 들어올리기']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''[\n",
    " '실내 자전거타기',\n",
    " '달리기',\n",
    " '다리 벌려 앞으로 상체 숙이기',\n",
    " '앉았다 일어서기',\n",
    " '버피운동',\n",
    " '몸통 비틀기',\n",
    " '다리 벌려 옆으로 상체 숙이기',\n",
    " '발목 얹고 다리 잡아당기기',\n",
    " '엎드려 양팔 및 다리 들어올리기',\n",
    " '몸통 들어올리기'\n",
    " ]'''\n",
    " \n",
    "def one_hot_encoder(exercise:str):\n",
    "    if exercise == \"실내 자전거타기\":\n",
    "        return [1,0,0,0,0,0,0,0,0,0]\n",
    "    elif exercise == \"달리기\":\n",
    "        return [0,1,0,0,0,0,0,0,0,0]\n",
    "    elif exercise == \"다리 벌려 앞으로 상체 숙이기\":\n",
    "        return [0,0,1,0,0,0,0,0,0,0]\n",
    "    elif exercise == \"앉았다 일어서기\":\n",
    "        return [0,0,0,1,0,0,0,0,0,0]\n",
    "    elif exercise == \"버피운동\":\n",
    "        return [0,0,0,0,1,0,0,0,0,0]\n",
    "    elif exercise == \"몸통 비틀기\":\n",
    "        return [0,0,0,0,0,1,0,0,0,0]\n",
    "    elif exercise == \"다리 벌려 옆으로 상체 숙이기\":\n",
    "        return [0,0,0,0,0,0,1,0,0,0]\n",
    "    elif exercise == \"발목 얹고 다리 잡아당기기\":\n",
    "        return [0,0,0,0,0,0,0,1,0,0]\n",
    "    elif exercise == \"엎드려 양팔 및 다리 들어올리기\":\n",
    "        return [0,0,0,0,0,0,0,0,1,0]\n",
    "    elif exercise == \"몸통 들어올리기\":\n",
    "        return [0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "    return [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "def one_hot_decoder(result):\n",
    "    if result == 1:\n",
    "        return \"실내 자전거타기\"\n",
    "    \n",
    "    if result == 2:\n",
    "        return \"달리기\"\n",
    "    \n",
    "    if result == 3:\n",
    "        return \"다리 벌려 앞으로 상체 숙이기\"\n",
    "    \n",
    "    if result == 4:\n",
    "        return \"앉았다 일어서기\"\n",
    "    \n",
    "    if result == 5:\n",
    "        return \"버피운동\"\n",
    "    \n",
    "    if result == 6:\n",
    "        return \"몸통 비틀기\"\n",
    "    \n",
    "    if result == 7:\n",
    "        return \"다리 벌려 옆으로 상체 숙이기\"\n",
    "    \n",
    "    if result == 8:\n",
    "        return \"발목 얹고 다리 잡아당기기\"\n",
    "    \n",
    "    if result == 9:\n",
    "        return \"엎드려 양팔 및 다리 들어올리기\"\n",
    "    \n",
    "    if result == 10:\n",
    "        return \"몸통 들어올리기\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainset = []\n",
    "\n",
    "for index, row in filtered_data.iterrows():\n",
    "    \n",
    "    trainset.append({\n",
    "        'data': torch.tensor([\n",
    "            sex_mapping(row[\"MBER_SEXDSTN_FLAG_CD\"]),\n",
    "            int(row['AGRDE_FLAG_NM'].replace(\"대\",\"\").replace(\" 이상\",\"\")),\n",
    "            weight_mapping(row['BMI_IDEX_GRAD_NM']),\n",
    "        ]).float().to(\"cpu\"),\n",
    "        'target': torch.tensor(one_hot_encoder(row['RECOMEND_MVM_NM'])).float().to(\"cpu\")\n",
    "    })\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3, 40)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(40, 10)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Loss: 2.450568199157715\n",
      "Epoch 2/200 Loss: 2.4576611518859863\n",
      "Epoch 3/200 Loss: 2.447147846221924\n",
      "Epoch 4/200 Loss: 2.3948259353637695\n",
      "Epoch 5/200 Loss: 2.4238290786743164\n",
      "Epoch 6/200 Loss: 2.454710006713867\n",
      "Epoch 7/200 Loss: 2.438900947570801\n",
      "Epoch 8/200 Loss: 2.4134368896484375\n",
      "Epoch 9/200 Loss: 2.4239301681518555\n",
      "Epoch 10/200 Loss: 2.285597085952759\n",
      "Epoch 11/200 Loss: 2.2929701805114746\n",
      "Epoch 12/200 Loss: 2.448291301727295\n",
      "Epoch 13/200 Loss: 2.4220352172851562\n",
      "Epoch 14/200 Loss: 2.424551248550415\n",
      "Epoch 15/200 Loss: 2.4280271530151367\n",
      "Epoch 16/200 Loss: 2.44999623298645\n",
      "Epoch 17/200 Loss: 1.8342286348342896\n",
      "Epoch 18/200 Loss: 2.440199851989746\n",
      "Epoch 19/200 Loss: 2.446322202682495\n",
      "Epoch 20/200 Loss: 2.4266276359558105\n",
      "Epoch 21/200 Loss: 2.433307647705078\n",
      "Epoch 22/200 Loss: 1.891574740409851\n",
      "Epoch 23/200 Loss: 2.434197425842285\n",
      "Epoch 24/200 Loss: 2.4257960319519043\n",
      "Epoch 25/200 Loss: 2.431765079498291\n",
      "Epoch 26/200 Loss: 1.8198254108428955\n",
      "Epoch 27/200 Loss: 2.272146463394165\n",
      "Epoch 28/200 Loss: 1.610417366027832\n",
      "Epoch 29/200 Loss: 2.438563346862793\n",
      "Epoch 30/200 Loss: 2.4283933639526367\n",
      "Epoch 31/200 Loss: 2.4308695793151855\n",
      "Epoch 32/200 Loss: 2.432710886001587\n",
      "Epoch 33/200 Loss: 2.4508495330810547\n",
      "Epoch 34/200 Loss: 2.4370033740997314\n",
      "Epoch 35/200 Loss: 2.43440842628479\n",
      "Epoch 36/200 Loss: 2.4211978912353516\n",
      "Epoch 37/200 Loss: 1.710081696510315\n",
      "Epoch 38/200 Loss: 1.7139226198196411\n",
      "Epoch 39/200 Loss: 2.2436413764953613\n",
      "Epoch 40/200 Loss: 2.417555332183838\n",
      "Epoch 41/200 Loss: 1.6016725301742554\n",
      "Epoch 42/200 Loss: 1.5785220861434937\n",
      "Epoch 43/200 Loss: 2.4163832664489746\n",
      "Epoch 44/200 Loss: 2.416522264480591\n",
      "Epoch 45/200 Loss: 1.5639910697937012\n",
      "Epoch 46/200 Loss: 2.4319536685943604\n",
      "Epoch 47/200 Loss: 2.433887481689453\n",
      "Epoch 48/200 Loss: 2.280390501022339\n",
      "Epoch 49/200 Loss: 2.4231677055358887\n",
      "Epoch 50/200 Loss: 2.4149680137634277\n",
      "Epoch 51/200 Loss: 2.4398603439331055\n",
      "Epoch 52/200 Loss: 2.100369930267334\n",
      "Epoch 53/200 Loss: 1.914103627204895\n",
      "Epoch 54/200 Loss: 2.4356372356414795\n",
      "Epoch 55/200 Loss: 2.4417920112609863\n",
      "Epoch 56/200 Loss: 2.4419500827789307\n",
      "Epoch 57/200 Loss: 2.417154312133789\n",
      "Epoch 58/200 Loss: 2.4336743354797363\n",
      "Epoch 59/200 Loss: 2.4410552978515625\n",
      "Epoch 60/200 Loss: 2.4336817264556885\n",
      "Epoch 61/200 Loss: 2.435123920440674\n",
      "Epoch 62/200 Loss: 2.24403977394104\n",
      "Epoch 63/200 Loss: 2.4243710041046143\n",
      "Epoch 64/200 Loss: 2.3229799270629883\n",
      "Epoch 65/200 Loss: 2.4449303150177\n",
      "Epoch 66/200 Loss: 2.4501869678497314\n",
      "Epoch 67/200 Loss: 2.447373867034912\n",
      "Epoch 68/200 Loss: 1.739182472229004\n",
      "Epoch 69/200 Loss: 2.424769878387451\n",
      "Epoch 70/200 Loss: 1.58183753490448\n",
      "Epoch 71/200 Loss: 2.428483724594116\n",
      "Epoch 72/200 Loss: 1.7541255950927734\n",
      "Epoch 73/200 Loss: 2.421532154083252\n",
      "Epoch 74/200 Loss: 2.3580851554870605\n",
      "Epoch 75/200 Loss: 2.2479827404022217\n",
      "Epoch 76/200 Loss: 2.438413381576538\n",
      "Epoch 77/200 Loss: 2.344111680984497\n",
      "Epoch 78/200 Loss: 2.0531210899353027\n",
      "Epoch 79/200 Loss: 1.6240776777267456\n",
      "Epoch 80/200 Loss: 2.1331942081451416\n",
      "Epoch 81/200 Loss: 2.1591758728027344\n",
      "Epoch 82/200 Loss: 2.4235572814941406\n",
      "Epoch 83/200 Loss: 1.7668893337249756\n",
      "Epoch 84/200 Loss: 2.4480090141296387\n",
      "Epoch 85/200 Loss: 2.418919563293457\n",
      "Epoch 86/200 Loss: 2.4227805137634277\n",
      "Epoch 87/200 Loss: 1.9700202941894531\n",
      "Epoch 88/200 Loss: 2.416457414627075\n",
      "Epoch 89/200 Loss: 2.416548728942871\n",
      "Epoch 90/200 Loss: 2.4154160022735596\n",
      "Epoch 91/200 Loss: 2.4430036544799805\n",
      "Epoch 92/200 Loss: 2.421470880508423\n",
      "Epoch 93/200 Loss: 2.4121134281158447\n",
      "Epoch 94/200 Loss: 2.419325351715088\n",
      "Epoch 95/200 Loss: 2.442026376724243\n",
      "Epoch 96/200 Loss: 2.44970703125\n",
      "Epoch 97/200 Loss: 1.9288251399993896\n",
      "Epoch 98/200 Loss: 2.048816680908203\n",
      "Epoch 99/200 Loss: 2.41996169090271\n",
      "Epoch 100/200 Loss: 2.0456836223602295\n",
      "Epoch 101/200 Loss: 2.0351922512054443\n",
      "Epoch 102/200 Loss: 1.5964840650558472\n",
      "Epoch 103/200 Loss: 2.037318468093872\n",
      "Epoch 104/200 Loss: 2.4146676063537598\n",
      "Epoch 105/200 Loss: 2.433131694793701\n",
      "Epoch 106/200 Loss: 2.1908419132232666\n",
      "Epoch 107/200 Loss: 2.4342989921569824\n",
      "Epoch 108/200 Loss: 2.4202847480773926\n",
      "Epoch 109/200 Loss: 1.8097081184387207\n",
      "Epoch 110/200 Loss: 1.8687093257904053\n",
      "Epoch 111/200 Loss: 1.8439778089523315\n",
      "Epoch 112/200 Loss: 2.4306201934814453\n",
      "Epoch 113/200 Loss: 2.430612087249756\n",
      "Epoch 114/200 Loss: 2.4126105308532715\n",
      "Epoch 115/200 Loss: 2.4504449367523193\n",
      "Epoch 116/200 Loss: 2.4384818077087402\n",
      "Epoch 117/200 Loss: 2.417067766189575\n",
      "Epoch 118/200 Loss: 2.4218435287475586\n",
      "Epoch 119/200 Loss: 2.436507225036621\n",
      "Epoch 120/200 Loss: 2.430607318878174\n",
      "Epoch 121/200 Loss: 1.8971798419952393\n",
      "Epoch 122/200 Loss: 2.4152703285217285\n",
      "Epoch 123/200 Loss: 2.420698404312134\n",
      "Epoch 124/200 Loss: 2.421713352203369\n",
      "Epoch 125/200 Loss: 2.4138121604919434\n",
      "Epoch 126/200 Loss: 2.1062304973602295\n",
      "Epoch 127/200 Loss: 1.5474236011505127\n",
      "Epoch 128/200 Loss: 2.180530071258545\n",
      "Epoch 129/200 Loss: 2.2097861766815186\n",
      "Epoch 130/200 Loss: 2.455866813659668\n",
      "Epoch 131/200 Loss: 2.437771797180176\n",
      "Epoch 132/200 Loss: 2.111750841140747\n",
      "Epoch 133/200 Loss: 2.4263763427734375\n",
      "Epoch 134/200 Loss: 2.413534164428711\n",
      "Epoch 135/200 Loss: 1.6114555597305298\n",
      "Epoch 136/200 Loss: 2.452533006668091\n",
      "Epoch 137/200 Loss: 2.441722869873047\n",
      "Epoch 138/200 Loss: 2.4209394454956055\n",
      "Epoch 139/200 Loss: 2.4208943843841553\n",
      "Epoch 140/200 Loss: 2.385362386703491\n",
      "Epoch 141/200 Loss: 2.431227922439575\n",
      "Epoch 142/200 Loss: 1.9604136943817139\n",
      "Epoch 143/200 Loss: 2.444814920425415\n",
      "Epoch 144/200 Loss: 2.418642997741699\n",
      "Epoch 145/200 Loss: 2.4210281372070312\n",
      "Epoch 146/200 Loss: 2.421110153198242\n",
      "Epoch 147/200 Loss: 2.439661741256714\n",
      "Epoch 148/200 Loss: 2.4509570598602295\n",
      "Epoch 149/200 Loss: 2.2649614810943604\n",
      "Epoch 150/200 Loss: 1.809808611869812\n",
      "Epoch 151/200 Loss: 1.5935022830963135\n",
      "Epoch 152/200 Loss: 2.431394577026367\n",
      "Epoch 153/200 Loss: 2.381120204925537\n",
      "Epoch 154/200 Loss: 2.4225423336029053\n",
      "Epoch 155/200 Loss: 2.1061692237854004\n",
      "Epoch 156/200 Loss: 2.413694381713867\n",
      "Epoch 157/200 Loss: 2.4222350120544434\n",
      "Epoch 158/200 Loss: 2.4554104804992676\n",
      "Epoch 159/200 Loss: 1.5356806516647339\n",
      "Epoch 160/200 Loss: 1.5520440340042114\n",
      "Epoch 161/200 Loss: 2.4477672576904297\n",
      "Epoch 162/200 Loss: 1.9292007684707642\n",
      "Epoch 163/200 Loss: 2.1422502994537354\n",
      "Epoch 164/200 Loss: 2.412708282470703\n",
      "Epoch 165/200 Loss: 2.4475767612457275\n",
      "Epoch 166/200 Loss: 2.42110013961792\n",
      "Epoch 167/200 Loss: 2.452413558959961\n",
      "Epoch 168/200 Loss: 2.0671069622039795\n",
      "Epoch 169/200 Loss: 2.4247658252716064\n",
      "Epoch 170/200 Loss: 2.4229657649993896\n",
      "Epoch 171/200 Loss: 2.002835750579834\n",
      "Epoch 172/200 Loss: 2.416171073913574\n",
      "Epoch 173/200 Loss: 1.9005788564682007\n",
      "Epoch 174/200 Loss: 2.413029670715332\n",
      "Epoch 175/200 Loss: 1.898911476135254\n",
      "Epoch 176/200 Loss: 1.5052992105484009\n",
      "Epoch 177/200 Loss: 2.453892230987549\n",
      "Epoch 178/200 Loss: 1.9295660257339478\n",
      "Epoch 179/200 Loss: 2.4373667240142822\n",
      "Epoch 180/200 Loss: 1.9390079975128174\n",
      "Epoch 181/200 Loss: 2.425143241882324\n",
      "Epoch 182/200 Loss: 2.4316225051879883\n",
      "Epoch 183/200 Loss: 2.423797845840454\n",
      "Epoch 184/200 Loss: 1.75791335105896\n",
      "Epoch 185/200 Loss: 2.4124274253845215\n",
      "Epoch 186/200 Loss: 1.7631893157958984\n",
      "Epoch 187/200 Loss: 2.4297492504119873\n",
      "Epoch 188/200 Loss: 1.6414605379104614\n",
      "Epoch 189/200 Loss: 2.4184226989746094\n",
      "Epoch 190/200 Loss: 2.209336280822754\n",
      "Epoch 191/200 Loss: 2.4309113025665283\n",
      "Epoch 192/200 Loss: 2.385017156600952\n",
      "Epoch 193/200 Loss: 2.4350759983062744\n",
      "Epoch 194/200 Loss: 2.450446844100952\n",
      "Epoch 195/200 Loss: 2.186572551727295\n",
      "Epoch 196/200 Loss: 2.1310596466064453\n",
      "Epoch 197/200 Loss: 2.438793897628784\n",
      "Epoch 198/200 Loss: 2.4538981914520264\n",
      "Epoch 199/200 Loss: 2.431879758834839\n",
      "Epoch 200/200 Loss: 2.4424500465393066\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model = DNN()\n",
    "\n",
    "# Move to GPU\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "epoch = 200\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # Reset the gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for this in random.sample(trainset, 100):\n",
    "        output = model(this['data'])\n",
    "        loss = criterion(output, this['target'])\n",
    "        loss.backward()\n",
    "        \n",
    "    optimizer.step()    \n",
    "    print(f\"Epoch {i+1}/{epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), \"model.pth\")\n",
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m one_hot_decoder(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\jyh2e\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m, in \u001b[0;36mDNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\jyh2e\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jyh2e\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "one_hot_decoder(model(torch.tensor([1, 30, 0]).float().cuda()).argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
